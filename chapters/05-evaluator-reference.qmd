# Evaluator Reference

## contains

Check if output contains a string:

```lua
{type = "contains", field = "output", value = "expected text"}
```

## regex

Match output against pattern:

```lua
{type = "regex", field = "phone", value = "\\(\\d{3}\\) \\d{3}-\\d{4}"}
```

## llm_judge

Use LLM to evaluate quality:

```lua
{
    type = "llm_judge",
    rubric = [[
Score 1.0 if the response is helpful and accurate.
Score 0.5 if partially correct.
Score 0.0 if incorrect or unhelpful.
    ]],
    model = "openai:gpt-4o-mini"
}
```

## tool_called

Verify tool was called:

```lua
{type = "tool_called", value = "search"}
{type = "tool_called", value = "search", min_value = 1, max_value = 3}
```

## state_check

Verify state variable:

```lua
{type = "state_check", field = "completed", value = true}
```

## json_schema

Validate JSON structure:

```lua
{
    type = "json_schema",
    field = "data",
    value = {
        type = "object",
        properties = {
            name = {type = "string"},
            age = {type = "number"}
        },
        required = {"name"}
    }
}
```

## range

Check numeric range:

```lua
{type = "range", field = "score", value = {min = 0, max = 100}}
```

## Thresholds

```lua
evaluations {
    thresholds = {
        min_success_rate = 0.90,
        max_cost_per_run = 0.01,
        max_duration = 10.0,
        max_tokens_per_run = 500
    }
}
```
